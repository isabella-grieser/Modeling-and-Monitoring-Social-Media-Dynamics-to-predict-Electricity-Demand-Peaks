\chapter{Modeling the Effects of Information on Power Grids}
\label{implementationall}
In this Chapter, a framework to model, analyze, and predict 
the effects of information on social media on the 
power grid will be introduced.
First, in Section \ref{simulationframeworksection}, 
the implementation of the simulation framework will be described.
First, in Section \ref{modelsocialnetwork}, the underlying social network model
used in the simulation model is described. Second, in Section 
\ref{modelinformationdiffusion}, the algorithm used to model
the information diffusion process in the social network is 
defined. Third, in Section \ref{rulebasedpowerconsumption}, 
the rules to model the additional power consumption is explained.
Additionally, in Section \ref{parameterestimationalgo},
an parameter estimation algorithm for the estimation of 
realistic values for the simulation parameters is introduced and 
the different steps of the framework are described.

\section{Implementation of the Simulation Framework}
\label{simulationframeworksection}
There are multiple components that are necessary to model the effects of 
both true and false information on critical infrastructures
such as the power grid. The development of the simulation framework
proposed in this work can be divided into three parts. First, a model 
to describe social media networks with characteristics similar 
or equal to real social media networks must be implemented.
Second, an algorithm to model the 
propagation of information over the network needs to be defined and 
implemented. Third, rules to estimate the changes in the power consumption 
are necessary.

\subsection{Social Network Modeling Approaches}
\label{modelsocialnetwork}
Two approaches for the creation of social media network graphs 
are considered in this Thesis.

First, a real social media network can be used as an example 
social network graph model. 
It is difficult to determine which individuals
control which accounts on social media networks due to privacy reasons.
Thus, it is not possible to create a social network graph that 
shows the real state of the social media network.
Therefore, datasets can be used to avoid dealing with privacy concerns. 
There are datasets available to analyze
real social media networks, e.g. the Stanford 
Large Network Dataset Collection \cite{snapnets}.
This collection is part of the Stanford Network Analysis Project (SNAP).
In this collection, there are datasets which show the network structure
of multiple social networks such as Facebook and Twitter.
The advantage of using real social networks as reference networks
for the simulation framework is that real networks fulfill the 
characteristics of social networks, since they
are based on real data and not randomly generated. The problem
is that with this approach, all simulations use either
one or few selected social network graph models. Since 
few different graph models are used for consecutive simulations,
it cannot be determined if the simulation results are based on the 
structure of the specific social network graph or if the results 
are seasonable for other social networks, i.e. the simulation results
may not be robust. Moreover, the size of the social network graph 
cannot be changed without changing its characteristics, such as 
the clustering coefficient. Since real social network graphs tend to be 
big (for example, the Twitter social network graph provided by SNAP
contains 81306 nodes and the Facebook social network graph contains 
4039 nodes), it may not be computationally feasible to use such graphs
as reference graphs.

The other approach would be to generate a graph that is not based
on any real social network, but can be used as a reference graph.
For this, the generated graph needs to fulfill the common
characteristics of social networks.
From the information provided in Section \ref{comparison-random-graphs}, 
it is visible that the Barabási–Albert (BA) graph is the random graph 
algorithm that fulfills the characteristics of social networks most effectively.
The reason is that BA graphs are both scale-free, have the small-world
characteristic and are also clustered.
Using a random graph to model the social media network has the advantage
that each simulation can be run with an new, generated graph model. 
Thus, the simulation results can be considered more robust. 
Furthermore, different graph sizes can be 
used in the simulation by changing the number of nodes to be generated.
Thus, using random graph algorithms is computationally easier since 
real social network graphs tend to be big.
The disadvantage is that random graph models may not fulfill
all characteristics of a social network. As mentioned in 
Section \ref{comparison-random-graphs}, the BA graph
does not create graphs with a clustering coefficient that is as 
high as that of real social networks. Nonetheless, the BA graph is
a graph that is often used to model social networks.
Since the usage of random graphs allows the usage
of unique graph models for the simulation and the size of the 
graph can be changed without changing the graph characteristics, 
the random graph generation approach was used to model social networks.

\subsection{Information Propagation Algorithm}
\label{modelinformationdiffusion}

There are multiple methods to model information diffusion in graph models.
In Section \ref{informationdiffsection}, three different information diffusion
model algorithms were introduced.
Both the information cascade model and the threshold model focus on the 
information diffusion process itself and not on the change of the behavior
of the single entities in the system. Thus, in these two models,
entities may have two states: 
\textit{Informed} or \textit{Not Informed}.
On the other hand, epidemiological models such as the SIR model
focus on the changes of the states of the entities in the system.
Therefore, it is possible to define different behaviors in the
information diffusion progress with epidemiological models. 
In this Thesis, the focus is on both how the information
propagates and how it could be combated using fact-checking.
This can be studied in more detail if entities can 
show different behaviors depending on what kind of information
they possess. Hence, an epidemiological model was chosen 
to model the information diffusion process.

Epidemiological models can be defined both by differential equations
or with graph-based algorithms.
The graph-based information diffusion model of \textit{Tambuscio et al.} 
\cite{sirsmodel}, which was introduced in Section \ref{epidemologicalmodels},
has the advantage of being able to model both the misinformation and fact-checking 
diffusion process in the same model. 
For this Thesis, the model of \cite{sirsmodel} was
modified to deal with a differing assumption.
The short-term effects of the information 
on the power grid are analyzed in this work. 
The recurring spread of the same misinformation
is not considered. This is done because it can be 
assumed that the initial spread of information leads to the 
greatest effect on the power grid since it is also assumed that this
spread leads to the greatest number of infected entities at
the time $t_{I_{max}}$. As a result, $p_{\mathrm{forget}}$ can be defined as 
$p_{\mathrm{forget}} = 0$.
This assumption leads to a modified state chart and probability
functions compared to \cite{sirsmodel}. The modified
state chart can be seen in Figure \ref{modifiedmodelstatechart}
and the modified probability functions can be seen in Table
\ref{modified-SIS-table-equations}. In Figure 
\ref{modifiedmodelstatechart}, it can be observed that 
there is no transition back to 
the state \textit{Susceptible} and all entities
will inevitably transition to the \textit{Recovered} state. 
In conclusion, the modified model can be categorized as a SIR model. 


\begin{figure}[!ht]
    \center
    \includegraphics[scale=.8]{figs/Tambuscio_modified.pdf}
    \caption{State chart for the modified model}
    \label{modifiedmodelstatechart}
\end{figure}

\begin{table}[ht!]
    \centering
    \begin{tabular}{|c  c |} 
     \hline
     & \\
     $\begin{aligned}
          p_i^S(t+1) &= (1-f_i-g_i)s_i^S(t) \\
          p_i^I(t+1) &= f_is_i^S(t) + (1-p_{verify})s_i^I(t) \\
          p_i^R(t+1) &= g_is_i^S(t) + p_{verify}s_i^I(t)+s_i^R(t)
        \end{aligned}$
      &
      $\begin{aligned}
          f_i(t) &= \beta \frac{n_i^I(t)(1+\alpha)}{n_i^I(t)(1+\alpha)+n_i^R(t)(1-\alpha)} \\
          g_i(t) &= \beta \frac{n_i^R(t)(1-\alpha)}{n_i^I(t)(1+\alpha)+n_i^R(t)(1-\alpha)} \\
        \end{aligned}$
       \\ 
       & \\
     \hline
    \end{tabular}
    \caption{Modified probability functions for the states}
    \label{modified-SIS-table-equations}
\end{table}

In the proposed information diffusion algorithm, 
the propagation of the information
progresses after each iteration step $i$. The algorithm divides the 
information propagation process in two parts. In the first part, 
the probabilities $p_n(t+1) = [p_i^S, p_i^I, p_i^R]$ for all nodes are
calculated. Then, the states of all nodes are changed 
based on the probabilities calculated in the previous step.
By separating both steps, it can be ensured that state changes
in the current iteration cannot influence the calculation
of the probability $p_n(t+1)$ of the other nodes.


\subsection{Rule-based Calculation of the Power Consumption}
\label{rulebasedpowerconsumption}

Next, given the social network modeling components given in 
Sections \ref{simulationframeworksection} and 
\ref{modelinformationdiffusion}, the rules to model the
power demand changes must be defined.
To model the demand changes in the simulation, multiple assumptions
are made. These are all listed below.

\begin{enumerate}
    \item Only \textit{infected} entities in 
    the social network graph can change their power demand. The reason is 
    that only changes in power demand due to information on social media
    are considered. Moreover, since \textit{susceptible} entities
    do not know of the information being spread on social media and 
    \textit{recovered} entities do not believe in the information anymore,
    these entities do not change their behavior, 
    unlike infected entities.
    \item Every entity in the social network graph 
    represents one household, and not a person. There are 
    two reasons for this assumption. First, it is assumed that 
    members of a single household are highly connected and if one 
    member of a household receives the information, all other members
    will be informed as well. Next, it is simpler to model power demand
    patterns on a household level than a personal level. The reason
    is that usually
    there is one of each appliance that consume a lot of electricity, 
    such as washing machines, ovens or dryers, in household.
    \item The time of infection of an entity may not
    always coincide with the time when the entity changes its 
    power consumption. This is due to the fact that in the information
    being spread on social media, there may be be points in time
    mentioned, hence leading to a delayed action response by the
    infected entities. An example scenario where such an assumption
    would be true is a false pricing information scenario 
    (Section \ref{contextmotivation}, Chapter 
    \ref{relatedworks}, Section \ref{demandresponsesection}).
    In this assumption, an entity may receive the information 
    at 3PM that the electricity price will be reduced 
    starting 5PM. Thus, the household already received the
    information and may be considered as infected in the social network
    graph, but it will not act on the information since changing
    its power demand would only benefit it starting 5PM. Thus, its 
    action will be delayed until 5PM. 
    \item There may not be an immediate change 
    in power demand even if an entity is both infected and the entity 
    is in the time window to take action. This means that even if
    both conditions are fulfilled, the entity may delay its action. 
    For example, given the false pricing information scenario, 
    even if it is 5PM, not all entities may immediately change their 
    behavior at 5PM, but some may start at 5:15PM, at 5:30PM, etc..
    The reason is that entities may not fully focus on the information
    that they received, but they will casually act on it while going 
    on with their lives. 
    Therefore, it is assumed that for each infected entity that can 
    take action with the probability $p_{\mathrm{power\_usage}}$, where 
    $p_{\mathrm{power\_usage}}$ is a system
    parameter that defines the probability where an entity
    starts changing its power demand when both other conditions are
    fulfilled. Furthermore, this condition is continuously checked 
    during the simulation until the condition is fulfilled.
    \item Even if both conditions that
    the entity is infected and the entity is in the time window 
    to take action are fulfilled, the entity may not take action.
    The idea is that even if an entity knows and believes
    in the information that is being spread and may also spread
    the information to its friends, it may not have the motivation 
    to act on the information.
    This assumption is given with the probability $p_{\mathrm{will\_act}}$.
    This assumption, contrary to the former assumption, is only
    checked once. If the condition is not fulfilled, the 
    entity will never act on the information, even if 
    it believes in the information.
    \item Some entities may not be able to participate due 
    to constraints that are outside of their control. For example, 
    one entity may be interested in participating in energy reduction
    program to reduce his energy costs, but is unable to do so since 
    the entity has an energy provider which does not offer such a program.
    Thus, even if the entity is infected and would like to act on the 
    information, contextual contraints do not allow him to participate in it.
    Nonetheless, the entity can still spread the information to its neighbors
    which might be able to act on it. 
    This availability to participate in the power demand change event is 
    given with the probability $p_{\mathrm{available}}$.
    \item There may be scenarios where 
    an entity both does not act on the information and does not 
    spread the information further. This would be the case for information
    which is considered not believeable or untrustworthy by the general
    population. These are called \textit{fringe} scenarios.
    \item If the total power demand of all entities exceeds a 
    critical threshold, a blackout occurs. The critical threshold
    is calculated as the maximum power usage $P_{max}$ 
    of the system if there is no excess power consumption times 
    a factor $a_{\mathrm{power\_threshold}}$, i.e.
    $a_{\mathrm{power\_threshold}} \cdot P_{max}$.
\end{enumerate}

The power demand changes should be comparable to reference data.
For this, load profiles are used as reference points to increase
or reduce power demand from consuming entities.
The load profile used for this Thesis is the standard load profile
for households created by the German Federal Association of Energy and 
Water Management (Bundesverband der Energie- und Wasserwirtschaft 
e. V. (BDEW)) \cite{meier1999reprasentative}.
For each infected household, the additional power usage is 
added on top of the power demand given by the load profile.
The additional power usage is defined by the appliances in each
household that are of relevance for the specific scenario. 

Given the assumptions and the system parameters, the simulation 
algorithm can thus be introduced. 
It is assumed that the social network graph used in the algorithm
is initialized with each node belonging to the \textit{Susceptible} class.
Moreover, it is assumed that $m$ nodes in the graph are declared
as available. Available nodes fulfill the constraint 
$p < p_{\mathrm{available}}$.
Then, in the initialization step of the simulation algorithm,
one node of the social network graph becomes infected,
thus acting as the initial source of information in the network.
Next, for each iteration step $i$, the total power demand 
based on the information in the social network is calculated 
as the sum of the power consumption of all nodes in the network. 
The equation to calculate the power consumption 
of a node $n$ if all excess power consumption conditions are fulfilled
is defined as 
$P_{n,i}=a \cdot P_{ref, i}+c_{\mathrm{offset}, n, i}$. 
If the conditions are not fulfilled, then the power consumption of the node 
equals the reference data $P_{ref, i}$, since it is 
not affected by the information and thus does not change it's
behaviour.
The variable $c_{\mathrm{offset}, n, i}$
is the sum of all appliances defined in the 
scenario that are used by the 
specific household $n$ at the iteration $i$.
Appliances are active and consume energy 
for a specific number of iterations $j$.
Afterwards, they are deactivated and hence do not contribute to 
the excess power consumption anymore.
Additionally, the power usage can be 
scaled by the factor $a$.

After the calculation of the power usage, the information is 
propagated in the network graph. 
After each iteration of the information propagation algorithm,
it is checked whether the infected individuals will change their behavior
based on the information. For this, it is first checked whether
the newly infected households will ever act on the information.
This constraint is calculated with the condition
$p<p_{\mathrm{will\_act}}$.
Next, it checks whether
an infected individual will delay its action, even if
all other constraints that hinder overconsumption of power
are fulfilled. This constraint is checked with the
condition $p<p_{\mathrm{power\_usage}}$. 
Then, it will start with the next iteration $i+1$.

Next, the main components of the simulation framework are shown
in Figure \ref{simframework}.
The simulation algorithm receives a social 
network graph, reference power consumption data with their 
corresponding dates, and 
a JSON file with the simulation parameters as input. 
Then, the simulation algorithm is executed.
The algorithm runs over
$i$ time steps. Next, the output of the simulation is the 
power demand over the time $[t, t+i]$ based on the
input parameters.

\begin{figure}[!ht]
    \center
    \includegraphics[scale=.5]{figs/Simulation-framework.pdf}
    \caption{Basic structure of the simulation framework}
    \label{simframework}
\end{figure}


\section{System Parameter Estimation Framework}
\label{parameterestimationalgo}

In Section \ref{simulationframeworksection}, a framework to simulate the effects 
of information on the power grid was introduced. However, the simulation results
may differ significantly depending on the parameters chosen for the simulation.
Furthermore, if the systems parameters are chosen arbitrarily,
the result of the simulation may be unrealistic. 
If the framework should produce realistic results and if
it should be used to possibly predict excess power consumption, an 
algorithm to estimate realistic values for the system parameters is needed.

In Section \ref{powerloadsection}, different variables that 
correlate with power load were introduced. These variables can be 
used to estimate realistic system parameters. For this Thesis, social media data is 
used to estimate the system parameters that control the 
information propagation. Since it is difficult to get personalized 
information of people on the internet due to data privacy reasons, the
social media data is assumed to be as generalized as possible. 
The identity of the users posting messages is hence assumed to be unknown. 
The only information
that is considered in this Section are the number of messages 
in a given timespan.

One method to estimate the propagation parameters of the SIR model 
is to view the problem as a minimization problem 
\cite{jin2013epidemiological}. The idea is to iteratively solve the differential 
equations of the SIR model, calculate the infection curve and minimize 
its difference to the true infection process that can be seen via the
social media data. A simplified diagram with the steps of the algorithm
can be seen in Figure \ref{paramestimationbasic}. The method proposed 
in this Thesis contains four different steps. 
The first three steps are used to gather and preprocess 
the data that is relevant for the parameter estimation process.
The last step is the parameter estimation algorithm.
The steps in the proposed method are:

\begin{enumerate}
    \item Find all relevant posts and messages on social media websites.
    This can be done either with natural language processing techniques or
    by using keyword search queries.
    \item Count the number of relevant social media posts over a given 
    timespan. This is done by counting the number of posts in given intervals, 
    e.g. the number of posts each 15 minutes. This data shows the
    popularity of a specific topic over time.
    \item Filter the data to smoothen it and to facilitate the
    parameter extraction process. The filtered data is $n_{\mathrm{posts}}(t)$.
    \item Initialize the minimization algorithm with predefined unknown
    variables and their initial values. Optimize the variables of the SIR model
    so that the difference of the infection curve of the SIR model
    and the number of posts over time is minimized. For each minimization
    step, the differential equations that define the infection progression
    in the system are solved.
\end{enumerate}

\begin{figure}[!ht]
    \center
    \includegraphics[scale=.5]{figs/parameter_estimation_process.pdf}
    \caption{Steps of the parameter estimation algorithm}
    \label{paramestimationbasic}
\end{figure}

To use the estimation algorithm, it is necessary to 
know the differential equations that describe the propagation 
process over time. However, the propagation algorithm
used in the simulation framework is not based on differential equations,
since it is a graph-based algorithm. 
Thus, mean-field theory is used to 
analyze the average interactions in the graph and thus to deduce 
the system-wide behavior of the propagation. 


To analyze the system-level behavior of the algorithm,
the parameters $S(t), I(t), R(t)$ are defined, where $S(t)$ is the number of
entities in the system with the status \textit{Susceptible} at the time $t$, 
$I(t)$ is the number of entities with the status \textit{Infected} and
$R(t)$ is the number of entities with the status \textit{Recovered}. 
Next, it is assumed that the states of the neighbors of a node $i$ 
at the time $t$ are randomly distributed given the probabilities 
$p^S(t), p^I(t), p^R(t)$, where $p^S(t) + p^I(t) + p^R(t) = 1$. 
The values $S(t), I(t), R(t)$ can be calculated
with the Equations \ref{SIR-system-amound-eqs:all-lines},
with $N$ being the total number of entities in the system.

\begin{subequations}
\begin{align}
    S(t) &=p^S(t)\cdot N \label{SIR-system-amound-eqs:1}\\
    I(t) &=p^I(t)\cdot N \label{SIR-system-amound-eqs:2}\\
    R(t) &=p^R(t)\cdot N \label{SIR-system-amound-eqs:3}
\end{align}
\label{SIR-system-amound-eqs:all-lines}
\end{subequations}

Given the Equations in \ref{SIR-system-amound-eqs:all-lines}, it is possible 
to calculate the changes $\Delta S(t), \Delta I(t), \Delta R(t)$ between each 
iteration step $t$ and $t+1$ in the system.
The changes can be calculated with the Equations \ref{SIR-diff-system-amount-eqs}.

\begin{subequations}
\begin{align}
    \frac{dS}{dt} \approx \frac{\Delta S(t)}{\Delta t} = \frac{\Delta S(t)}{1} = \Delta S(t) &=\Delta p^S(t)\cdot N \label{SIR-diff-system-amount-eqs:1}\\
    \frac{dI}{dt} \approx \frac{\Delta I(t)}{\Delta t} = \frac{\Delta I(t)}{1} = \Delta I(t) &=\Delta p^I(t)\cdot N \label{SIR-diff-system-amount-eqs:2}\\
    \frac{dR}{dt} \approx \frac{\Delta R(t)}{\Delta t} = \frac{\Delta R(t)}{1} = \Delta R(t) &=\Delta p^R(t)\cdot N \label{SIR-diff-system-amount-eqs:3}
\end{align}
\label{SIR-diff-system-amount-eqs}
\end{subequations}

Since it is assumed that the states are randomly distributed over the system, 
the state of the node $i$ is assumed to be randomly distributed.
Thus, the state $s_i$ of the node $i$ can be defined as in Equation 
\ref{state-node-equation}.

\begin{equation}
    s_i = [s_i^S, s_i^I, s_i^R]T = [p^S(t), p^I(t), p^R(t)]^T
    \label{state-node-equation}
\end{equation}

Then, it can be assumed that the median degree of a BA graph
will be equal to the number of edges generated when adding a new 
node to the random graph.
This assumption can be made since the graph follows the power law distribution,
which defines that few nodes have a high amount of connections to other nodes
and a high number of nodes have comparatively low number of connections to 
other nodes. Since the minimum number of edges that a node can have 
in the Barabási–Albert random graph generation algorithm is $k$, it 
can be assumed that $\forall i \to c_i=k$.
With this, the functions $g_i,f_i$ defined in Table 
\ref{modified-SIS-table-equations}
can be generalized by assuming that the average number of neighbors 
of a specific state $K$ can be calculated as $k\cdot p^K(t)$. For this
assumption, $p^K(t)$ is the 
average probability that a node is in the state $K$.
Therefore, the functions $g_i,f_i$ can be generalized to the
Equations \ref{generalized-functions-g-f}.

\begin{subequations}
\begin{align}
    f_i(t) &= \beta \frac{n_i^I(t)(1+\alpha)}{n_i^I(t)(1+\alpha)+n_i^R(t)(1-\alpha)} 
    \nonumber\\
    \to f(t) &= k\beta \frac{p^I(t)(1+\alpha)}{p^I(t)(1+\alpha)+p^R(t)(1-\alpha)}
    \label{generalized-function-f} \\
    g_i(t) &= \beta \frac{n_i^R(t)(1-\alpha)}{n_i^I(t)(1+\alpha)+n_i^R(t)(1-\alpha)} 
    \nonumber \\
    \to g(t) &= k\beta \frac{p^R(t)(1-\alpha)}{p^I(t)(1+\alpha)+p^R(t)(1-\alpha)}
    \label{generalized-function-g}
\end{align}
\label{generalized-functions-g-f}
\end{subequations}

With these assumptions, the system-level equations for the SIR model can 
be deduced. First, the function for $\Delta R(t)$ can be infered
as shown in Equation \ref{delta-r-deduction-eqs}.

\begin{align}
    p^R(t+1) &= g \cdot p^S(t) + p_{\mathrm{verify}}\cdot p^I(t) + p^R(t) \nonumber\\
    p^R(t+1) - p^R(t) &= g \cdot p^S(t) + p_{\mathrm{verify}}\cdot p^I(t) \nonumber\\
    \Delta R(t) = (p^R(t+1) - p^R(t))\cdot N 
    &= N(g \cdot p^S(t) + p_{\mathrm{verify}}\cdot p^I(t)) \nonumber\\
    &= N(g \cdot \frac{S(t)}{N} + p_{\mathrm{verify}}\cdot \frac{I(t)}{N} ) \nonumber\\
    &= g \cdot S(t) + p_{\mathrm{verify}}\cdot I(t) \nonumber\\
    &= k\beta \frac{p^R(t)(1-\alpha)}{p^I(t)(1+\alpha)+p^R(t)(1-\alpha)} 
    \cdot S(t) + p_{\mathrm{verify}}\cdot I(t) \nonumber\\
    &= k\beta \frac{\frac{R(t)}{N}(1-\alpha)}{\frac{I(t)}{N}(1+\alpha)+\frac{R(t)}{N}(1-\alpha)} 
    \cdot S(t) + p_{\mathrm{verify}}\cdot I(t) \nonumber\\
    \Delta R(t) &= \frac{k\beta}{N} \frac{R(t)(1-\alpha)}{I(t)(1+\alpha)+R(t)(1-\alpha)} 
    \cdot S(t) + p_{\mathrm{verify}}\cdot I(t) \label{delta-r-deduction-eqs}
\end{align}

The equation for $\Delta I(t)$ can be infered in a similar manner to $\Delta R(t)$.
The deduction steps for $\Delta I(t)$ are shown in Equation 
\ref{delta-i-deduction-eqs}.

\begin{align}
    p^I(t+1) &= f \cdot p^S(t) + (1 - p_{{\mathrm{verify}}})\cdot p^I(t) \nonumber\\
     &= f \cdot p^S(t) + p^I(t) - p_{{\mathrm{verify}}}\cdot p^I(t) \nonumber\\
    p^I(t+1) - p^I(t) &= f \cdot p^S(t) - p_{{\mathrm{verify}}}\cdot p^I(t) \nonumber\\
    \Delta I(t) = (p^I(t+1) - p^I(t)) \cdot N 
    &= N (f \cdot p^S(t) - p_{{\mathrm{verify}}}\cdot p^I(t)) \nonumber\\
    &= N (f \cdot \frac{S(t)}{N}  - p_{{\mathrm{verify}}}\cdot \frac{I(t)}{N}) \nonumber\\
    &= f \cdot S(t) - p_{{\mathrm{verify}}}\cdot I(t) \nonumber\\
    &=  k\beta \frac{p^I(t)(1+\alpha)}{p^I(t)(1+\alpha)+p^R(t)(1-\alpha)}
     \cdot S(t) - p_{{\mathrm{verify}}}\cdot I(t) \nonumber\\
    &=  k\beta \frac{\frac{B(t)}{N}(1+\alpha)}{\frac{I(t)}{N}(1+\alpha)+\frac{R(t)}{N}(1-\alpha)}
     \cdot S(t) - p_{{\mathrm{verify}}}\cdot I(t) \nonumber\\
     \Delta I(t) &=  \frac{k\beta}{N} \frac{I(t)(1+\alpha)}{I(t)(1+\alpha)+R(t)(1-\alpha)}
     \cdot S(t) - p_{{\mathrm{verify}}}\cdot I(t) \label{delta-i-deduction-eqs}
\end{align}

Last, given that there are no entities entering or leaving the system,
it can be assumed that the state changes in the system add to zero,
hence $\Delta S(t)+ \Delta I(t)+ \Delta R(t) = 0$. With this, the equation for
$\Delta S$ can be deduced as shown in Equation \ref{delta-s-deduction-eqs}.

\begin{align}
    \Delta S(t) &= - \Delta I(t) - \Delta R(t) \nonumber\\
     &= -\frac{k\beta}{N} \frac{I(t)(1+\alpha)}{B(t)(1+\alpha)+R(t)(1-\alpha)}
     \cdot S(t) + p_{{\mathrm{verify}}}\cdot I(t) \nonumber\\
      & -\frac{k\beta}{N} \frac{R(t)(1-\alpha)}{B(t)(1+\alpha)+R(t)(1-\alpha)} 
      \cdot S(t) - p_{{\mathrm{verify}}}\cdot I(t) \nonumber\\
      &= -\frac{k\beta}{N} \frac{I(t)(1+\alpha) + R(t)(1-\alpha)}{I(t)(1+\alpha)+R(t)(1-\alpha)}
      \cdot S(t) + p_{{\mathrm{verify}}}\cdot I(t) - p_{{\mathrm{verify}}}\cdot I(t) \nonumber\\
      &= -\frac{k\beta}{N} \frac{I(t)(1+\alpha) + R(t)(1-\alpha)}{I(t)(1+\alpha)+R(t)(1-\alpha)}
      \cdot S(t) \nonumber\\
      \Delta S(t) &= -\frac{k\beta}{N} \cdot S(t) \label{delta-s-deduction-eqs}
\end{align}


Summarized, the differential equations that are used for the parameter estimation 
algorithm can be seen in Equation \ref{all-deduced-diff-equations}.

\begin{subequations}
    \begin{align}
        \frac{dS}{dt} \approx \Delta S(t) &= -\frac{k\beta}{N} \cdot S(t) \\
        \frac{dI}{dt} \approx \Delta I(t) &=  \frac{k\beta}{N} \frac{I(t)(1+\alpha)}{I(t)(1+\alpha)+R(t)(1-\alpha)}
        \cdot S(t) - p_{{\mathrm{verify}}}\cdot I(t) \\
        \frac{dR}{dt} \approx \Delta R(t) &= \frac{k\beta}{N} \frac{R(t)(1-\alpha)}{I(t)(1+\alpha)+R(t)(1-\alpha)} 
        \cdot S(t) + p_{{\mathrm{verify}}}\cdot I(t)
\end{align}
\label{all-deduced-diff-equations}
\end{subequations}


The differential equations have eight variables 
$N, k, \alpha, \beta, p_{{\mathrm{verify}}}, S(t_0), I(t_0), R(t_0)$
that need to be optimized. One unknown variable is
the true size of the system $N$, since the size of the relevant
subgroup of the social media network which is affected by the information is 
not known.
Next, the average degree of the social network k is unknown,
since the specific characteristics of the network of the
subgroup infected or susceptible to the information is not known.
Then, the three propagation parameters $\alpha, \beta, p_{{\mathrm{verify}}}$ 
are not known and need to be infered. 
Last, the initial values $S(t_0), I(t_0), R(t_0)$ are also not known.

\section{Prediction of Excess Power Consumption Events}

Next, in conjunction with the simulation framework described in 
\ref{simulationframeworksection}, the prediction of
critical excess power consumption events are possible. 
For this, the works described in Sections \ref{simulationframeworksection} 
and \ref{parameterestimationalgo} are combined to a single framework.
The steps in the proposed framework are shown in 
Figure \ref{basicpredicitonframework}.
First, the data used for the parameter estimation process is gathered and
preprocessed. Second, this data is used for the parameter estimation algorithm. 
Third, the simulation framework is configured
with the parameters estimated in the previous step. For this, 
$\alpha, \beta, p_{{\mathrm{verify}}}, S(t_0), I(t_0), R(t_0)$ 
are used to configure the 
information propagation process implemented in the simulation framework.
The variables $N$ and $k$ are used to configure the random graph generation
characteristics. Since it is assumed that the median degree
$k$ equals the number of edges generated for each incoming node,
$k$ can be used as an input for the random graph algorithm. 
However, this assumption
only works if the total number of nodes in the random graph is $N$.
Since social networks tend to be big, it can be assumed that 
$N$ is of high value and that a graph of such a size may lead to technical 
issues such as a high computation time. Therefore, smaller values such as $N'$
can be used as long as the ration between the number of nodes and the
number of edges for the incoming nodes are the same, hence the constraint
$\frac{k}{N}=\frac{k'}{N'}$ needs to be fulfilled for a Barabási–Albert
graph $G'(N',k')$. The simulation runs with both
the parameters for the information propagation and for the random graph. 
Last, given the results of the simulation and the power threshold
that define the upper limit of the power grid infrastructure, it
is checked whether the power demand exceeds the predefined threshold.


\begin{figure}[!ht]
    \center
    \includegraphics[scale=.6]{figs/full_prediction_framework.pdf}
    \caption{Steps of the proposed framework to predict power demand surges
    started by information shared in social media networks}
    \label{basicpredicitonframework}
\end{figure}


% als letzte Idee: herleiten, ab welchen p_verify I_max unter 
% ein bestimmtes threshhold ist